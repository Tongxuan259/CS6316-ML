{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Machine Learning HW1\n",
    "**<center> Name: Tongxuan Tian, Computing ID: nua3jz, Date: 09/21/2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1 Solution\n",
    "To prove that Bayes predictor is the optimal predictor, we need to prove that for any predictor $h$, we have:\n",
    "$$L_D(f_D) \\leq L_D(f_h)$$\n",
    "Since $L_D(f_D) = P_{x, y \\in D}(h(x) \\neq y)$, we can write is as:\n",
    "$$ L_D(f_D)  =\\begin{cases}\n",
    " \\text{$\\mathbb{P}[y=+1|x]$ if $h(x)=+1$} \\;\\;\\\\\n",
    " \\text{$\\mathbb{P}[y=-1|x]$ if $h(x)=-1$} \\;\\;\n",
    "\\end{cases}$$\n",
    "Since it is a binary classification problem, we can know that $ \\mathbb{P}[y=+1|x]+\\mathbb{P}[y=-1|x] = 1 $, thus we have:\n",
    "$$ L_D(f_D)  =\\begin{cases}\n",
    " \\text{$\\mathbb{P}[y=+1|x]$     if $h(x)=+1$} \\;\\;\\\\\n",
    " \\text{$1 - \\mathbb{P}[y=+1|x]$ if $h(x)=-1$} \\;\\;\n",
    "\\end{cases}$$  \n",
    "Then we should discuss on a case-by-case basis.\n",
    "1. When $ \\mathbb{P}[y=+1|x] < 1 - \\mathbb{P}[y=-1|x] $, we have $\\mathbb{P}[y=+1|x] < \\frac{1}{2}$ and $\\mathbb{P}[y=-1|x] > \\frac{1}{2}$, which means $f_D(x) = -1$.  \n",
    "Then we have $L_D(f_D) = \\mathbb{P}[y=+1|x] < \\frac{1}{2}$. Thus, we can get that $L_D(f_D) \\leq L_D(f_h)$.\n",
    "2. Similarly, when $ \\mathbb{P}[y=+1|x] > 1 - \\mathbb{P}[y=-1|x] $, we have $\\mathbb{P}[y=+1|x] > \\frac{1}{2}$ and $\\mathbb{P}[y=-1|x] < \\frac{1}{2}$, which means $f_D(x) = +1$.  \n",
    "Then we have $L_D(f_D) = \\mathbb{P}[y=-1|x] < \\frac{1}{2}$. Thus, we can get that $L_D(f_D) \\leq L_D(f_h)$.  \n",
    "\n",
    "Based on above, we can know that for any $h$, we always have $L_D(f_D) \\leq L_D(f_h)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2 Solution\n",
    "**Question (a)**  \n",
    "It is obvious that the decision boundary of this Bayes predictor is the intersection point of 2 normal distribution. Thus, we can get it through the equation below:\n",
    "$$ \\frac{1}{2}N(x;0,1) = \\frac{1}{2}N(x;\\frac{2}{3}\\pi, 0.5) $$  \n",
    "We can extend it as:  \n",
    "$$ \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}} = \\frac{1}{0.5\\sqrt{2\\pi}}e^{-\\frac{(x-\\frac{2}{3}\\pi)^2}{2(0.5)^2}} $$\n",
    "And we can get the solution to this equality is $ x \\approx 1.2396 $, which is the decision boundary $b_{Bayes}$ we want for this Bayes predictor.  \n",
    "\n",
    "**Question (b)**  \n",
    "The true error of this Bayes predictor is actually the intersection area of two Gaussian distributions. Thus, we can written the true error as:  \n",
    "$$ L_{D}(f_D) = \\int_{-\\infty}^{b_{Bayes}} \\frac{1}{2}N(x;0,1) dx + \\int_{b_{Bayes}}^{+\\infty} \\frac{1}{2}N(x;\\frac{2}{3}\\pi, 0.5) dx$$  \n",
    "We can easily get that $$ L_{D}(f_D) \\approx 0.0756 $$  \n",
    "Thus, the true error of this Bayes predictor $ L_{D}(f_D) $ is 0.0756  \n",
    "\n",
    "**Question (c)**  \n",
    "To find the best hypothesis $ h^* \\in H $, we need to find the minimal true error. Run code block below to get the best hypothesis and corresponding decision boundary (May take about 30s~)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hypothesis i = 496\n",
      "Decision boundary b = 1.24\n",
      "True error L = 0.07561630298491684\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.integrate import quad\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "h_num = 1200\n",
    "\n",
    "def find_intsec(mu1, sigma1, mu2, sigma2):\n",
    "    pdf1 = lambda x: norm.pdf(x, loc=mu1, scale=sigma1)\n",
    "    pdf2 = lambda x: norm.pdf(x, loc=mu2, scale=sigma2)\n",
    "    \n",
    "    intersection_func = lambda x: pdf1(x) - pdf2(x)\n",
    "    intersection_x = fsolve(intersection_func, (mu1 + mu2) / 2)\n",
    "    \n",
    "    return intersection_x\n",
    "\n",
    "\n",
    "def integrate_normal(mu, sigma, lower_limit, upper_limit):\n",
    "    pdf = lambda x: norm.pdf(x, loc=mu, scale=sigma)\n",
    "    result, _ = quad(pdf, lower_limit, upper_limit)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def find_best_hypothsis():\n",
    "    min_error = np.inf\n",
    "    best_h = 1\n",
    "    for i in range(1, h_num + 1):\n",
    "        h = i / 400\n",
    "        \n",
    "        true_error = 0.5 * integrate_normal((2/3)*np.pi, 0.5, -np.inf, h) \\\n",
    "            + 0.5 * integrate_normal(0, 1, h, np.inf)\n",
    "\n",
    "        if true_error < min_error:\n",
    "            min_error = true_error\n",
    "            best_h = i\n",
    "\n",
    "    return best_h, min_error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    best_h, min_error = find_best_hypothsis()\n",
    "\n",
    "    print(f\"Best hypothesis i = {best_h}\")\n",
    "    print(f\"Decision boundary b = {best_h / 400}\")\n",
    "    print(f\"True error L = {min_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (d)**  \n",
    "Based on the result of the above code block, we can know that true error $L_D(h^*) \\approx 0.0756$  \n",
    "\n",
    "**Question (e)**  \n",
    "Similarly, we need to minimize the empirical error on data we generated to find the best hypothesis $h_S$.  Run code block below to get the best hypothesis and corresponding decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hypothesis: i = 500\n",
      "Decision boundary: bs = 1.25\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import normal\n",
    "\n",
    "N = 100                 # sample size\n",
    "comp_idx = [1,-1]       # lable\n",
    "x_pos, x_neg = [], []   # observations\n",
    "\n",
    "for i in range(N):\n",
    "    x_neg.append(normal(0, 1))\n",
    "    x_pos.append(normal((2 / 3) * np.pi, 0.5))\n",
    "\n",
    "min_error = 1\n",
    "best_h = 1\n",
    "for i in range(1, h_num + 1):\n",
    "        \n",
    "    h = i / 400\n",
    "    em_error = 0\n",
    "\n",
    "    for x_p in x_pos:\n",
    "        if x_p <= h:\n",
    "            em_error += 1\n",
    "    \n",
    "    for x_n in x_neg:\n",
    "        if x_n >= h:\n",
    "            em_error += 1\n",
    "\n",
    "    em_risk  = em_error / (2 * N)\n",
    "\n",
    "    if em_risk < min_error:\n",
    "        min_error = em_risk\n",
    "        best_h = i\n",
    "\n",
    "print(f\"Best hypothesis: i = {best_h}\")\n",
    "print(f\"Decision boundary: bs = {best_h / 400}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (f)**  \n",
    "Similar to **question (b)**, since we know the decision boundary $b_S$ of this hypothesis, we written its corresponding true error as:  \n",
    "$$ L_D(h_S)= \\int_{-\\infty}^{b_S} \\frac{1}{2}N(x;0,1) dx + \\int_{b_S}^{+\\infty} \\frac{1}{2}N(x;\\frac{2}{3}\\pi, 0.5) dx$$  \n",
    "Run code block below to get the true error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True error: L = 0.07563979723904254\n"
     ]
    }
   ],
   "source": [
    "true_error = 0.5 * integrate_normal((2 / 3)*np.pi, 0.5, -np.inf, best_h / 400) \\\n",
    "    + 0.5 * integrate_normal(0, 1, best_h / 400, np.inf)\n",
    "print(f\"True error: L = {true_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3 Solution\n",
    "Code block below is the python implementation of Perceptron Algorithm. Run it to get the final $w^{(t)}$. Make sure **data.txt** is in the same folder of this jupyter notebook file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 = -3.5199999999999987, w2 = -1.2400000000000002, b = 2.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_txt(file_path):\n",
    "    \n",
    "    file = open(file_path,'r')\n",
    "    file_data = file.readlines()\n",
    "    x1, x2, y = [], [], []\n",
    "\n",
    "    for row in file_data:\n",
    "        tmp_list = row.replace('\\n', '').split('\\t')\n",
    "        x1.append(float(tmp_list[0]))\n",
    "        x2.append(float(tmp_list[1]))\n",
    "        y.append(float(tmp_list[-1]))\n",
    "\n",
    "    return {'x1': x1, 'x2': x2,}, {'y': y}\n",
    "\n",
    "def perception(input, lable):\n",
    "    assert len(input['x1']) == len(lable['y'])\n",
    "    x1 = np.array(input['x1'])\n",
    "    x2 = np.array(input['x2'])\n",
    "\n",
    "    x = np.column_stack((np.transpose(x1), np.transpose(x2),\n",
    "                          np.transpose(np.ones_like(x2))))\n",
    "\n",
    "    y = np.array(lable['y'])\n",
    "    w = np.zeros_like(x[0]) \n",
    "\n",
    "    while True:\n",
    "        flag = 0\n",
    "        for t in range(len(x)):\n",
    "            i = t % len(x)\n",
    "            \n",
    "            if (y[i] * np.sum(w * x[i])) <= 0:\n",
    "                w = w + y[i] * x[i]\n",
    "            \n",
    "            if (y * np.sum(w * x, axis=1) < 0).sum() <= 0:\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag:\n",
    "            break\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = './data.txt'\n",
    "    input, lable = read_txt(data_path)\n",
    "    w = perception(input=input, lable=lable)\n",
    "    print(f\"w1 = {w[0]}, w2 = {w[1]}, b = {w[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the final $w^{(t)}$ is $[-3.52, -1.24]$ and bias is $2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4 Solution\n",
    "The gradient of $L(h_w, S)$ with respect to $w$ is\n",
    "$$ \\frac{dL(h_w, S)}{dw} = \\frac{1}{m} \\sum_{i=1}^{n} \\frac{  \\frac{d(1+exp(-y_i\\langle{w}, x_i\\rangle))}{dw}}{(1+exp(-y_i\\langle{w}, x_i\\rangle)) \\ln {e}}\n",
    "$$  \n",
    "$$=\\frac{1}{m} \\sum_{i=1}^{n} \\frac{exp(-y_i\\langle{w}, x_i\\rangle)}{(1+exp(-y_i\\langle{w}, x_i\\rangle)) \\ln {e}} \\frac{d(-y_i\\langle{w}, x_i\\rangle)}{dw}\n",
    "$$  \n",
    "$$=\\frac{1}{m} \\sum_{i=1}^{n} \\frac{exp(-y_i\\langle{w}, x_i\\rangle)}{(1+exp(-y_i\\langle{w}, x_i\\rangle)) \\ln {e}} (-y_i x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5 Solution\n",
    "Since $L_{l2}(h_w, S)$ is a convex function, the optimal solution can be given by the following equation:  \n",
    "$$ \\frac{dL_{l2}(h_w, S)}{dw}=0 $$ \n",
    "$$2\\sum_{i=1}^{n}(\\langle{w_i}, x_i\\rangle-y_i)x_i+2 \\lambda w=0$$\n",
    "$$w(\\sum_{i=1}^{n}(x_i x_i^T)+ \\lambda)=\\sum_{i=1}^{n}{y_i x_i}$$  \n",
    "$$w=(\\sum_{i=1}^{n}(x_i x_i^T)+ \\lambda)^{-1}\\sum_{i=1}^{n}{y_i x_i}$$  \n",
    "By denoting $(\\sum_{i=1}^{n}(x_i x_i^T)+ \\lambda)^{-1} = A$ and $b = \\sum_{i=1}^{n}{y_i x_i}$, we can rewrite the equality above as below in short:  \n",
    "$$w=(A+ \\lambda I)^{-1}b$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
